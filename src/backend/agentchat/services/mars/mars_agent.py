import asyncio
import inspect
import json
import time
import typing
from typing import List, Dict, Any

from langchain_core.messages import BaseMessage, SystemMessage, ToolCall, AIMessage, ToolMessage
from langchain_core.tools import Tool, BaseTool
from loguru import logger
from openai.types.chat import ChatCompletionMessageToolCall
from openai.types.chat.chat_completion_message_tool_call import Function
from pydantic import BaseModel, create_model, Field

from agentchat.core.models.manager import ModelManager
from agentchat.prompts.chat_prompt import DEFAULT_CALL_PROMPT
from agentchat.services.mars.mars_tools import Mars_Call_Tool
from agentchat.services.mars.mars_tools.autobuild import construct_auto_build_prompt
from agentchat.settings import app_settings


class MarsConfig(BaseModel):
    user_id: str

class MarsEnum:
    AutoBuild_Agent = 1
    Retrieval_Knowledge = 2
    AI_News = 3
    Deep_Search = 4



class MarsAgent:

    def __init__(self, mars_config: MarsConfig):
        self.mars_tools = None
        self.mars_config = mars_config

        # æµå¼äº‹ä»¶é˜Ÿåˆ—
        self.event_queue = asyncio.Queue()
        self.step_counter_lock = asyncio.Lock()
        self.step_counter = 1

    async def emit_event(self, data: Dict[Any, Any]):
        """å‘é€æµå¼äº‹ä»¶"""
        event = {
            "type": "event",
            "timestamp": time.time(),
            "data": data
        }
        await self.event_queue.put(event)

    async def init_mars_agent(self):

        self.mars_tools = await self.set_mars_tools()

        await self.set_language_model()
    # async def set_knowledges(self):
    #     pass
    #
    # async def set_mcp_agents(self):
    #     pass
    #
    # async def set_plugin_tools(self):
    #     pass

    async def set_mars_tools(self) -> List[BaseTool]:
        # TODOï¼šå› ä¸ºToolå¿…é¡»ç»‘å®šfuncï¼Œä½†ä¸ç”¨ï¼ŒåŠ ä¸ªtestå‡½æ•°
        def test_func():
            pass

        mars_tools = []
        tools_name = [name for name, coroutine in Mars_Call_Tool.items()]
        for name in tools_name:
            mars_tools.append(Tool.from_function(name=name, description=Mars_Call_Tool[name].__doc__, func=test_func, coroutine=Mars_Call_Tool[name]))
        return mars_tools

    async def set_language_model(self):
        # æ™®é€šå¯¹è¯æ¨¡åž‹
        self.conversation_model = ModelManager.get_conversation_model()

        # æ”¯æŒFunction Callæ¨¡åž‹
        self.tool_invocation_model = ModelManager.get_tool_invocation_model()

        # æŽ¨ç†æ¨¡åž‹
        self.reasoning_model = ModelManager.get_reasoning_model()

    async def call_tools_messages(self, messages: List[BaseMessage]) -> AIMessage:
        """è°ƒç”¨å·¥å…·é€‰æ‹©ï¼Œæ·»åŠ æµå¼äº‹ä»¶"""

        call_tool_messages: List[BaseMessage] = []

        # åªæœ‰ä¸€æ¬¡å·¥å…·è°ƒç”¨
        tools_schema = []
        for tool in self.mars_tools:
            if tool.name == "auto_build_agent":
                auto_build_prompt = await construct_auto_build_prompt(self.mars_config.user_id)
                tool.coroutine.__doc__ = tool.coroutine.__doc__.replace("{{{user_configs_placeholder}}}", auto_build_prompt)
            tools_schema.append(function_to_args_schema(tool.coroutine))

        self.tool_invocation_model.bind_tools(tools_schema)

        system_message = SystemMessage(content=DEFAULT_CALL_PROMPT)
        call_tool_messages.append(system_message)

        call_tool_messages.extend(messages)

        response = await self.tool_invocation_model.ainvoke(call_tool_messages)
        # åˆ¤æ–­æ˜¯å¦æœ‰å·¥å…·å¯è°ƒç”¨
        if response.tool_calls:
            openai_tool_calls = response.tool_calls

            response.tool_calls = convert_langchain_tool_calls(response.tool_calls)


            return AIMessage(
                content=response.content,
                tool_calls=response.tool_calls,
            )
        else:
            return AIMessage(content="æ²¡æœ‰å‘½ä¸­å¯ç”¨çš„å·¥å…·")

    async def execute_tool_message(self, messages: List[BaseMessage]):
        """æ‰§è¡Œå·¥å…·ï¼Œæ·»åŠ æµå¼äº‹ä»¶"""
        tool_calls = messages[-1].tool_calls


        for tool_call in tool_calls:

            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            tool_call_id = tool_call["id"]
            use_tool = None
            for mars_tool in self.mars_tools:
                if mars_tool.name == tool_name:
                    use_tool = mars_tool

            # æ›´æ–°ç”¨æˆ·å‚æ•°åˆ°å·¥å…·å‚æ•°é‡Œé¢
            tool_args.update({"user_id": self.mars_config.user_id})

            async for chunk in use_tool.coroutine(**tool_args):
                # TODO: å°†chunkç±»åž‹æ”¹æˆç»Ÿä¸€çš„response_chunk
                chunk["type"] = "response_chunk"
                yield chunk



    async def ainvoke_stream(self, messages: List[BaseMessage]):
        # ç”¨äºŽä¸­æ–­æŽ¨ç†æ¨¡åž‹è¾“å‡ºçš„äº‹ä»¶
        reasoning_interrupt = asyncio.Event()
        # ç”¨äºŽå­˜æ”¾Mars Agentè¾“å‡ºçš„é˜Ÿåˆ—
        mars_output_queue = asyncio.Queue()


        async def run_mars_agent():
            """
            è¿è¡ŒMars Agentï¼Œæ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶å°†å…¶è¾“å‡ºæ”¾å…¥é˜Ÿåˆ—ã€‚
            """
            try:
                # 1. åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
                call_tool_message = await self.call_tools_messages(messages)
                if not call_tool_message.tool_calls:
                    # å¦‚æžœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ”¾å…¥Noneä½œä¸ºç»“æŸä¿¡å·å¹¶ç›´æŽ¥è¿”å›ž
                    await mars_output_queue.put(None)
                    return

                messages.append(call_tool_message)

                # 2. æ‰§è¡Œå·¥å…·å¹¶å¤„ç†è¾“å‡º
                first_chunk = True
                mars_task_first_chunk = {
                    "type": "response_chunk",
                    "time": time.time(),
                    "data": "#### ä»»åŠ¡å·²ç»å®Œæˆï¼Œæˆ‘å¼€å§‹ä¸ºä½ è¾“å‡ºç»“æžœ âœ…\n"
                }

                async for chunk in self.execute_tool_message(messages):
                    if first_chunk:
                        # å½“ç¬¬ä¸€ä¸ªå·¥å…·è¾“å‡ºäº§ç”Ÿæ—¶ï¼Œè®¾ç½®ä¸­æ–­äº‹ä»¶
                        reasoning_interrupt.set()
                        # çŸ­æš‚ç­‰å¾…ï¼Œä»¥ç¡®ä¿æŽ¨ç†ä»»åŠ¡æœ‰æ—¶é—´å“åº”ä¸­æ–­ä¿¡å·
                        await asyncio.sleep(0.01)
                        first_chunk = False
                        # å‘é€æš‚åœæŽ¨ç†ï¼Œå¼€å§‹ä»»åŠ¡å›žå¤çš„ä¿¡æ¯
                        await mars_output_queue.put(mars_task_first_chunk)

                    # å°†å·¥å…·çš„è¾“å‡ºå—æ”¾å…¥é˜Ÿåˆ—
                    await mars_output_queue.put(chunk)
                
                # æ‰€æœ‰å·¥å…·è¾“å‡ºå¤„ç†å®Œæ¯•ï¼Œæ”¾å…¥Noneä½œä¸ºç»“æŸä¿¡å·
                await mars_output_queue.put(None)
            except Exception as e:
                logger.error(f"Mars Agent æ‰§è¡Œå‡ºé”™: {e}")
                # å³ä½¿å‡ºé”™ä¹Ÿè¦ç¡®ä¿æ”¾å…¥ç»“æŸä¿¡å·
                await mars_output_queue.put(None)

        async def run_reasoning_model():
            """
            è¿è¡ŒæŽ¨ç†æ¨¡åž‹ï¼Œæµå¼è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œå¹¶éšæ—¶å“åº”ä¸­æ–­äº‹ä»¶ã€‚
            """
            try:
                response = await self.reasoning_model.astream(messages)
                async for chunk in response:
                    # åœ¨æ¯æ¬¡è¾“å‡ºå‰æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–­
                    if reasoning_interrupt.is_set():
                        break

                    delta = chunk.choices[0].delta
                    if hasattr(delta, "reasoning_content") and delta.reasoning_content is not None:
                        yield {
                            "type": "reasoning_chunk",
                            "time": time.time(),
                            "data": delta.reasoning_content
                        }

                    if hasattr(delta, "content") and delta.content:
                        yield {
                            "type": "response_chunk",
                            "time": time.time(),
                            "data": delta.content
                        }
            except Exception as e:
                logger.error(f"æŽ¨ç†æ¨¡åž‹æµå¼è¾“å‡ºé”™è¯¯: {e}")

        # --- ä¸»æ‰§è¡Œæµç¨‹ ---

        # 1. ç«‹å³è¿”å›žåˆå§‹ä¿¡æ¯
        yield {
            "type": "response_chunk",
            "time": time.time(),
            "data": "#### çŽ°åœ¨å¼€å§‹ï¼Œæˆ‘ä¼šè¾¹æ¢³ç†æ€è·¯è¾¹å®Œæˆè¿™é¡¹ä»»åŠ¡ðŸ˜Š\n"
        }

        # 2. åœ¨åŽå°å¯åŠ¨Mars Agentä»»åŠ¡
        mars_task = asyncio.create_task(run_mars_agent())

        # 3. é¦–å…ˆï¼Œæµå¼è¾“å‡ºæŽ¨ç†æ¨¡åž‹çš„æ€è€ƒè¿‡ç¨‹ï¼Œç›´åˆ°è¢«ä¸­æ–­
        async for reasoning_chunk in run_reasoning_model():
            yield reasoning_chunk

        # 4. æŽ¨ç†è¿‡ç¨‹ç»“æŸåŽï¼Œå¼€å§‹å¤„ç†å¹¶è¾“å‡ºMars Agentçš„ç»“æžœ
        while True:
            mars_chunk = await mars_output_queue.get()
            if mars_chunk is None:  # æ”¶åˆ°ç»“æŸä¿¡å·
                break
            yield mars_chunk

        # 5. ç¡®ä¿Mars Agentä»»åŠ¡å·²å½»åº•å®Œæˆ
        await mars_task



# å°†OpenAIçš„function callæ ¼å¼è½¬æˆLangchainæ ¼å¼åšé€‚é…
def convert_langchain_tool_calls(tool_calls: List[ChatCompletionMessageToolCall]):
    langchain_tool_calls: List[ToolCall] = []

    for tool_call in tool_calls:
        langchain_tool_calls.append(
            ToolCall(id=tool_call.id, args=json.loads(tool_call.function.arguments), name=tool_call.function.name))

    return langchain_tool_calls




# å°†å‡½æ•°è½¬æˆfunction schemaæ ¼å¼
def function_to_args_schema(func) -> dict:
    """
    Converts a Python function into a JSON-serializable dictionary
    that describes the function's signature, including its name,
    description, and parameters.

    Args:
        func: The function to be converted.

    Returns:
        A dictionary representing the function's signature in JSON format.
    """
    sig = inspect.signature(func)
    fields = {
        name: (param.annotation, ... if param.default is inspect.Parameter.empty else param.default)
        for name, param in sig.parameters.items()
    }
    model = create_model(func.__name__, **fields)
    schema = model.model_json_schema()

    return {
        "type": "function",
        "function": {
            "name": func.__name__,
            "description": func.__doc__ or "",
            "parameters": schema
        },
    }


