import asyncio
import copy
import time
from loguru import logger
from typing import List, Dict, Any
from pydantic import BaseModel
from langgraph.types import Command
from langchain_core.tools import BaseTool
from langchain.agents.middleware import wrap_tool_call, after_model, ToolCallLimitMiddleware
from langgraph.prebuilt.tool_node import ToolCallRequest
from langchain.agents import AgentState, create_agent
from langchain_core.messages import BaseMessage, AIMessage, ToolMessage, AIMessageChunk

from agentchat.api.services.usage_stats import UsageStatsService
from agentchat.core.callbacks.usage_metadata import UsageMetadataCallbackHandler
from agentchat.core.models.manager import ModelManager
from agentchat.schema.usage_stats import UsageStatsAgentType
from agentchat.services.mars.mars_tools import MarsTool
from agentchat.services.mars.mars_tools.autobuild import construct_auto_build_prompt


class MarsConfig(BaseModel):
    user_id: str

class MarsEnum:
    AutoBuild_Agent = 1
    Retrieval_Knowledge = 2
    AI_News = 3
    Deep_Search = 4


class MarsAgent:

    def __init__(self, mars_config: MarsConfig):
        self.mars_tools = None
        self.mars_config = mars_config


    async def init_mars_agent(self):
        self.mars_tools = await self.setup_mars_tools()
        await self.setup_language_model()
        self.middlewares = await self.setup_middlewares()

        self.react_agent = self.setup_react_agent()

    async def setup_mars_tools(self) -> List[BaseTool]:
        mars_tools = []
        for name in MarsTool:
            if name == "auto_build_agent":
                auto_build_prompt = await construct_auto_build_prompt(self.mars_config.user_id)
                mars_tool = copy.deepcopy(MarsTool[name])
                mars_tool.description = mars_tool.description.replace("{{{user_configs_placeholder}}}", auto_build_prompt)
                mars_tools.append(mars_tool)
            else:
                mars_tools.append(MarsTool[name])
        return mars_tools

    async def setup_language_model(self):
        # æ™®é€šå¯¹è¯æ¨¡å‹
        self.conversation_model = ModelManager.get_conversation_model()

        # æ”¯æŒFunction Callæ¨¡å‹
        self.tool_invocation_model = ModelManager.get_tool_invocation_model()

        # æ¨ç†æ¨¡å‹
        self.reasoning_model = ModelManager.get_reasoning_model()

    def setup_react_agent(self):
        return create_agent(
            model=self.conversation_model,
            tools=self.mars_tools,
            middleware=self.middlewares,
        )

    async def setup_middlewares(self):
        tool_call_limiter = ToolCallLimitMiddleware(
            thread_limit=1
        )

        @after_model
        async def handler_after_model(
            state: AgentState,
            runtime,
        ) -> dict[str, Any] | None:
            last_message = state["messages"][-1]
            if not last_message.tool_calls:
                await self.mars_output_queue.put(None)
            return None

        @wrap_tool_call
        async def handler_tool_call(
            request: ToolCallRequest,
            handler,
        ) -> ToolMessage | Command:
            request.tool_call["args"].update({"user_id": self.mars_config.user_id})
            tool_result = await handler(request)
            return ToolMessage(content=tool_result, tool_call_id=request.tool_call["id"])

        return [tool_call_limiter, handler_after_model, handler_tool_call]


    async def ainvoke_stream(self, messages: List[BaseMessage]):
        # ç”¨äºä¸­æ–­æ¨ç†æ¨¡å‹è¾“å‡ºçš„äº‹ä»¶
        self.reasoning_interrupt = asyncio.Event()
        # ç”¨äºå­˜æ”¾Mars Agentè¾“å‡ºçš„é˜Ÿåˆ—
        self.mars_output_queue = asyncio.Queue()

        self.is_call_tool = False

        callback = UsageMetadataCallbackHandler()
        async def run_mars_agent():
            """
            è¿è¡ŒMars Agentï¼Œæ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶å°†å…¶è¾“å‡ºæ”¾å…¥é˜Ÿåˆ—ã€‚
            """
            async for token, chunk in self.react_agent.astream(
                input={"messages": messages},
                config={"callbacks": [callback]},
                stream_mode=["custom"]
            ):
                self.is_call_tool = True
                await self.mars_output_queue.put(chunk)

            await self.mars_output_queue.put(None)

        async def run_reasoning_model():
            """
            è¿è¡Œæ¨ç†æ¨¡å‹ï¼Œæµå¼è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œå¹¶éšæ—¶å“åº”ä¸­æ–­äº‹ä»¶ã€‚
            """
            try:
                response = await self.reasoning_model.astream(messages)
                async for chunk in response:
                    # åœ¨æ¯æ¬¡è¾“å‡ºå‰æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–­
                    if self.reasoning_interrupt.is_set():
                        break

                    delta = chunk.choices[0].delta
                    if hasattr(delta, "reasoning_content") and delta.reasoning_content is not None:
                        yield {
                            "type": "reasoning_chunk",
                            "time": time.time(),
                            "data": delta.reasoning_content
                        }

                    if hasattr(delta, "content") and delta.content:
                        if self.is_call_tool: # å¦‚æœè°ƒç”¨Marså·¥å…·çš„è¯ ä½¿ç”¨å·¥å…·é‡Œé¢çš„ä¿¡æ¯è¿›è¡Œå›ç­”
                            break
                        else:
                            yield {
                                "type": "response_chunk",
                                "time": time.time(),
                                "data": delta.content
                            }
            except Exception as e:
                logger.error(f"æ¨ç†æ¨¡å‹æµå¼è¾“å‡ºé”™è¯¯: {e}")

        # --- ä¸»æ‰§è¡Œæµç¨‹ ---

        # ç«‹å³è¿”å›åˆå§‹ä¿¡æ¯
        yield {
            "type": "response_chunk",
            "time": time.time(),
            "data": "#### ç°åœ¨å¼€å§‹ï¼Œæˆ‘ä¼šè¾¹æ¢³ç†æ€è·¯è¾¹å®Œæˆè¿™é¡¹ä»»åŠ¡ğŸ˜Š\n"
        }

        # åœ¨åå°å¯åŠ¨Mars Agentä»»åŠ¡
        mars_task = asyncio.create_task(run_mars_agent())

        # é¦–å…ˆï¼Œæµå¼è¾“å‡ºæ¨ç†æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼Œç›´åˆ°è¢«ä¸­æ–­
        async for reasoning_chunk in run_reasoning_model():
            yield reasoning_chunk

        # æ¨ç†è¿‡ç¨‹ç»“æŸåï¼Œå¼€å§‹å¤„ç†å¹¶è¾“å‡ºMars Agentçš„ç»“æœ
        while True:
            mars_chunk = await self.mars_output_queue.get()
            if mars_chunk is None:  # æ”¶åˆ°ç»“æŸä¿¡å·
                break
            yield mars_chunk

        # ç¡®ä¿Mars Agentä»»åŠ¡å·²å½»åº•å®Œæˆ
        await mars_task

    async def _record_agent_token_usage(self, response: AIMessage | AIMessageChunk | BaseMessage, model):
        if response.usage_metadata:
            await UsageStatsService.create_usage_stats(
                model=model,
                user_id=self.mars_config.user_id,
                agent=UsageStatsAgentType.mars_agent,
                input_tokens=response.usage_metadata.get("input_tokens"),
                output_tokens=response.usage_metadata.get("output_tokens")
            )


