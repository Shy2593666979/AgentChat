import random
import time
from datetime import datetime
from typing import Optional
from urllib.parse import urljoin
from uuid import uuid4
from loguru import logger

from agentchat.services.aliyun_oss import aliyun_oss
from agentchat.services.mars.ai_news.detial_news import yield_crawl_detail_ai_news
from agentchat.settings import app_settings
from agentchat.utils.file_utils import get_aliyun_oss_base_path




async def crawl_ai_news(user_input: str,
                        provide_download: bool = False,
                        user_id: Optional[str] = None):
    """
    å¸®åŠ©ç”¨æˆ·è·å–ä¸€ä¸ªAIæ—¥æŠ¥, å¦‚æœç”¨æˆ·æœ‰éœ€æ±‚ï¼Œå¯ä»¥æä¾›ä¸€ä¸ªå¯ä¸‹è½½çš„Markdownä¸‹è½½é“¾æ¥

    params:
        user_input: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        provide_download: æ˜¯å¦ç»™ç”¨æˆ·æä¾›æ–‡ä»¶çš„ä¸‹è½½é“¾æ¥

    return:
        è¿”å›æ¥æ—¥æŠ¥å†…å®¹, æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚åˆ¤æ–­æ˜¯å¦åŒ…å«å¯ä¸‹è½½çš„Markdowné“¾æ¥
    """
    news_response = ""
    async for chunk in yield_crawl_detail_ai_news():
        if chunk.get("type") == "response_chunk":
            news_response += chunk.get("data")
        yield chunk

    if provide_download:
        yield {
            "type": "tool_chunk",
            "time": time.time(),
            "data": "\n\næ¥ä¸‹æ¥æˆ‘è¦å¼€å§‹ç”Ÿæˆä¸€ä»½å®Œæ•´çš„Markdownæ–‡ä»¶\n"
        }

        try:
            file_name = f"AIæ—¥æŠ¥-{datetime.today().date()}.md"

            oss_object_name = get_aliyun_oss_base_path(file_name)
            sign_url = urljoin(app_settings.aliyun_oss["base_url"], oss_object_name)

            aliyun_oss.sign_url_for_get(sign_url)
            aliyun_oss.upload_file(oss_object_name, news_response)

            yield {
                "type": "tool_chunk",
                "time": time.time(),
                "data": f"æ–‡ä»¶å·²ç»ç”Ÿæˆå®Œæ¯•, è¯·ç‚¹å‡»ä¸‹è½½æŸ¥çœ‹ [AIæ—¥æŠ¥ğŸ“°]({sign_url}) \n"
            }
        except Exception as err:
            logger.error(f"ç”Ÿæˆæ—¥æŠ¥æ–‡ä»¶å¤±è´¥:{err}")

