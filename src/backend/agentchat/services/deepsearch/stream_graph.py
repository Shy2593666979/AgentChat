import asyncio
from typing import Dict, List, AsyncGenerator, Optional, Callable
import contextvars
from langchain_core.messages import HumanMessage
from langchain_core.messages import AIMessage
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from tavily import TavilyClient
from loguru import logger
import json
from dataclasses import dataclass

from agentchat.core.models.manager import ModelManager
from agentchat.services.deepsearch.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
)
from agentchat.services.deepsearch.configuration import Configuration
from agentchat.services.deepsearch.prompts import (
    get_current_date,
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
)
from agentchat.settings import app_settings

# åˆå§‹åŒ–Tavilyå®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=app_settings.tools.tavily["api_key"])

# ä½¿ç”¨contextvarsæ¥ä¼ é€’æµå¼è¾“å‡ºå›è°ƒï¼Œæ”¯æŒå¹¶å‘
stream_callback: contextvars.ContextVar[Optional[Callable]] = contextvars.ContextVar('stream_callback', default=None)


@dataclass
class StreamOutput:
    """æµå¼è¾“å‡ºæ•°æ®ç»“æ„"""
    type: str  # streaming, start, complete, error, info
    node: str
    content: str
    metadata: Optional[Dict] = None


async def stream_output(node_name: str, content: str, output_type: str = "content", metadata: Optional[Dict] = None):
    """å‘é€æµå¼è¾“å‡º"""
    callback = stream_callback.get()
    if callback:
        try:
            output = StreamOutput(
                type=output_type,
                node=node_name,
                content=content,
                metadata=metadata or {}
            )
            await callback(output)
        except Exception as e:
            logger.error(f"æµå¼è¾“å‡ºå›è°ƒå¤±è´¥: {e}")


class StreamingGraph:
    """æµå¼è¾“å‡ºçš„æ™ºèƒ½ä½“ç±»ï¼Œæ¯ä¸ªå®ä¾‹ç‹¬ç«‹ç®¡ç†è‡ªå·±çš„æµå¼è¾“å‡º"""

    def __init__(self):
        self.output_queue = asyncio.Queue()
        self.conversation_model = ModelManager.get_conversation_model()

    async def _stream_callback(self, output: StreamOutput):
        """å†…éƒ¨æµå¼è¾“å‡ºå›è°ƒ"""
        try:
            await self.output_queue.put({
                "type": output.type,
                "node": output.node,
                "content": output.content,
                "metadata": output.metadata
            })
        except asyncio.QueueFull:
            logger.warning("æµå¼è¾“å‡ºé˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒè¾“å‡º")

    async def generate_query(self, state: OverallState, config: RunnableConfig) -> QueryGenerationState:
        """LangGraphèŠ‚ç‚¹ï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€‚"""
        configurable = Configuration.from_runnable_config(config)

        if state.get("initial_search_query_count") is None:
            state["initial_search_query_count"] = configurable.number_of_initial_queries

        current_date = get_current_date()
        research_topic = get_research_topic(state["messages"])

        formatted_prompt = f"""
        {query_writer_instructions.format(
            current_date=current_date,
            research_topic=research_topic,
            number_queries=state["initial_search_query_count"],
        )}

        è¯·ç”¨JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¤ä¸ªé”®:
        {{
            "rationale": "ç®€è¦è§£é‡Šè¿™äº›æŸ¥è¯¢ä¸ç ”ç©¶ä¸»é¢˜çš„ç›¸å…³æ€§",
            "query": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", ...]
        }}
        """

        await stream_output("generate_query", f"å¼€å§‹ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œä¸»é¢˜ï¼š{research_topic}", "start")

        content = ""
        async for chunk in self.conversation_model.astream(formatted_prompt):
            content += chunk.content
        
        try:
            content = content.replace("```json", "").replace("```", "")
            result = json.loads(content)
            queries = result.get("query", [])
            if not queries:
                queries = [research_topic]

            await stream_output("generate_query", f"ç”Ÿæˆäº†{len(queries)}ä¸ªæœç´¢æŸ¥è¯¢", "complete", {"queries": queries})
            return {"search_query": queries}
        except Exception as e:
            logger.error(f"è§£ææŸ¥è¯¢ç”Ÿæˆç»“æœå¤±è´¥: {e}")
            await stream_output("generate_query", "è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜ä½œä¸ºæŸ¥è¯¢", "error")
            return {"search_query": [research_topic]}

    def continue_to_web_research(self, state: QueryGenerationState):
        """LangGraphèŠ‚ç‚¹ï¼Œå°†æœç´¢æŸ¥è¯¢å‘é€åˆ°ç½‘ç»œç ”ç©¶èŠ‚ç‚¹ã€‚"""
        return [
            Send("web_research", {"search_query": search_query, "id": int(idx)})
            for idx, search_query in enumerate(state["search_query"])
        ]

    async def web_research(self, state: WebSearchState, config: RunnableConfig) -> OverallState:
        """LangGraphèŠ‚ç‚¹ï¼Œä½¿ç”¨Tavilyæœç´¢APIæ‰§è¡Œç½‘ç»œç ”ç©¶ã€‚"""
        search_query = state["search_query"]
        query_id = state["id"]

        await stream_output("web_research", f"å¼€å§‹æœç´¢ï¼š{search_query}", "start",
                      {"query_id": query_id})
        logger.info(f"ğŸ” æ‰§è¡Œæœç´¢: {search_query}")

        try:
            response = await asyncio.to_thread(
                tavily_client.search,
                query=search_query,
                max_results=10,
                time_range="month",
                include_raw_content="markdown",
                country="china"
            )
            
            formatted_results = self.format_tavily_results(response)

            sources = []
            for idx, result in enumerate(response.get("results", [])):
                source_id = f"{query_id}-{idx}"
                source_url = result.get("url", "")
                source_title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                sources.append({
                    "short_url": f"https://search.result/{source_id}",
                    "value": source_url,
                    "label": source_title
                })

            result_count = len(response.get('results', []))
            await stream_output("web_research", f"æ‰¾åˆ° {result_count} ä¸ªæœç´¢ç»“æœ", "complete",
                          {"result_count": result_count, "query_id": query_id})
            logger.info(f"âœ… æ‰¾åˆ° {result_count} ä¸ªç»“æœ")

            return {
                "sources_gathered": sources,
                "search_query": [search_query],
                "web_research_result": [formatted_results],
            }
        except Exception as e:
            error_msg = f"æœç´¢å¤±è´¥: {str(e)}"
            await stream_output("web_research", error_msg, "error", {"query_id": query_id})
            logger.error(f"âŒ {error_msg}")
            return {
                "sources_gathered": [],
                "search_query": [search_query],
                "web_research_result": [error_msg],
            }

    def format_tavily_results(self, response: Dict) -> str:
        """æ ¼å¼åŒ–Tavilyæœç´¢ç»“æœ"""
        if not response.get("results"):
            return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"

        formatted = []
        for idx, result in enumerate(response["results"]):
            url = result.get("url", "")
            title = result.get("title", "")
            content = result.get("content", "")
            formatted.append(f"[{title}]({url})\nå†…å®¹: {content}")

        return "\n\n".join(formatted)

    async def reflection(self, state: OverallState, config: RunnableConfig) -> ReflectionState:
        """LangGraphèŠ‚ç‚¹ï¼Œè¯†åˆ«çŸ¥è¯†ç¼ºå£å¹¶ç”Ÿæˆæ½œåœ¨çš„åç»­æŸ¥è¯¢ã€‚"""
        configurable = Configuration.from_runnable_config(config)
        state["research_loop_count"] = state.get("research_loop_count", 0) + 1

        await stream_output("reflection", "å¼€å§‹åˆ†æç ”ç©¶ç»“æœï¼Œè¯†åˆ«çŸ¥è¯†ç¼ºå£", "start",
                      {"loop_count": state["research_loop_count"]})

        current_date = get_current_date()
        research_topic = get_research_topic(state["messages"])
        summaries = "\n\n---\n\n".join(state["web_research_result"])

        formatted_prompt = f"""
        {reflection_instructions.format(
            current_date=current_date,
            research_topic=research_topic,
            summaries=summaries,
        )}
        """

        response = await self.conversation_model.ainvoke(formatted_prompt)
        content = response.content

        try:
            content = content.replace("```json", "").replace("```", "")
            result = json.loads(content)
            is_sufficient = result.get("is_sufficient", True)
            knowledge_gap = result.get("knowledge_gap", "")
            follow_up_queries = result.get("follow_up_queries", [])

            status = "è¶³å¤Ÿ" if is_sufficient else "ä¸è¶³å¤Ÿ"
            await stream_output("reflection", f"åˆ†æå®Œæˆï¼šå½“å‰ä¿¡æ¯{status}", "complete",
                          {"is_sufficient": is_sufficient, "follow_up_count": len(follow_up_queries)})

            logger.info(f"ğŸ“Š åæ€ç»“æœ: {status}")
            if not is_sufficient:
                logger.info(f"ğŸ’­ çŸ¥è¯†ç¼ºå£: {knowledge_gap}")
                logger.info(f"ğŸ”„ åç»­æŸ¥è¯¢: {follow_up_queries}")
                await stream_output("reflection", f"éœ€è¦è¿›è¡Œ{len(follow_up_queries)}ä¸ªåç»­æŸ¥è¯¢", "info")

            return {
                "is_sufficient": is_sufficient,
                "knowledge_gap": knowledge_gap,
                "follow_up_queries": follow_up_queries,
                "research_loop_count": state["research_loop_count"],
                "number_of_ran_queries": len(state["search_query"]),
            }
        except Exception as e:
            logger.error(f"è§£æåæ€ç»“æœå¤±è´¥: {e}")
            await stream_output("reflection", "è§£æåæ€ç»“æœå¤±è´¥ï¼Œé»˜è®¤ä¸ºè¶³å¤Ÿ", "error")
            return {
                "is_sufficient": True,
                "knowledge_gap": "",
                "follow_up_queries": [],
                "research_loop_count": state["research_loop_count"],
                "number_of_ran_queries": len(state["search_query"]),
            }

    def evaluate_research(self, state: ReflectionState, config: RunnableConfig) -> OverallState:
        """LangGraphè·¯ç”±å‡½æ•°ï¼Œç¡®å®šç ”ç©¶æµç¨‹ä¸­çš„ä¸‹ä¸€æ­¥ã€‚"""
        configurable = Configuration.from_runnable_config(config)
        max_research_loops = (
            state.get("max_research_loops")
            if state.get("max_research_loops") is not None
            else configurable.max_research_loops
        )

        if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
            stream_output("evaluate_research", "ç ”ç©¶å®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ", "complete")
            logger.info("âœ… ç ”ç©¶å®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            return "finalize_answer"
        else:
            stream_output("evaluate_research", "ç»§ç»­ç ”ç©¶ï¼Œæ‰§è¡Œåç»­æŸ¥è¯¢", "continue")
            logger.info("ğŸ”„ ç»§ç»­ç ”ç©¶ï¼Œæ‰§è¡Œåç»­æŸ¥è¯¢")
            return [
                Send(
                    "web_research",
                    {
                        "search_query": follow_up_query,
                        "id": state["number_of_ran_queries"] + int(idx),
                    },
                )
                for idx, follow_up_query in enumerate(state["follow_up_queries"])
            ]

    async def finalize_answer(self, state: OverallState, config: RunnableConfig):
        """LangGraphèŠ‚ç‚¹ï¼Œå®Œæˆç ”ç©¶æ‘˜è¦ã€‚"""
        await stream_output("finalize_answer", "å¼€å§‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ\n", "start")

        current_date = get_current_date()
        research_topic = get_research_topic(state["messages"])
        summaries = "\n---\n\n".join(state["web_research_result"])

        formatted_prompt = f"""
        {answer_instructions.format(
            current_date=current_date,
            research_topic=research_topic,
            summaries=summaries,
        )}
        """

        content = ""
        async for chunk in self.conversation_model.astream(formatted_prompt):
            content += chunk.content
            await stream_output("finalize_answer", chunk.content, "streaming")

        logger.info("ğŸ¯ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå®Œæˆ")
        await stream_output("finalize_answer", "æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå®Œæˆ", "complete")

        unique_sources = []
        for source in state["sources_gathered"]:
            if source["short_url"] in content:
                content = content.replace(source["short_url"], source["value"])
                unique_sources.append(source)

        return {
            "messages": [AIMessage(content=content)],
            "sources_gathered": unique_sources,
        }

    def create_graph(self) -> StateGraph:
        """åˆ›å»ºLangGraph"""
        builder = StateGraph(OverallState, config_schema=Configuration)

        builder.add_node("generate_query", self.generate_query)
        builder.add_node("web_research", self.web_research)
        builder.add_node("reflection", self.reflection)
        builder.add_node("finalize_answer", self.finalize_answer)

        builder.add_edge(START, "generate_query")
        builder.add_conditional_edges(
            "generate_query", self.continue_to_web_research, ["web_research"]
        )
        builder.add_edge("web_research", "reflection")
        builder.add_conditional_edges(
            "reflection", self.evaluate_research, ["web_research", "finalize_answer"]
        )
        builder.add_edge("finalize_answer", END)

        return builder.compile(name="pro-search-agent")

    async def run_with_streaming(self, messages: List[HumanMessage]) -> AsyncGenerator[Dict, None]:
        """ä½¿ç”¨å¼‚æ­¥æµå¼è¾“å‡ºè¿è¡Œæ™ºèƒ½ä½“"""
        graph = self.create_graph()
        
        async def graph_task():
            token = stream_callback.set(self._stream_callback)
            try:
                # ä½¿ç”¨ astream å¼‚æ­¥æµå¼è°ƒç”¨
                async for chunk in graph.astream({"messages": messages}):
                    # astream å·²ç»å°†æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºæµå¼åŒ–ï¼Œ
                    # æˆ‘ä»¬å¯ä»¥é€šè¿‡å›è°ƒæŠŠå®ƒä»¬é€åˆ°é˜Ÿåˆ—
                    pass # astreamæœ¬èº«å°±ä¼šè§¦å‘èŠ‚ç‚¹ä¸­çš„stream_output
            except Exception as e:
                logger.error(f"å›¾æ‰§è¡Œå¤±è´¥: {e}")
                await self._stream_callback(StreamOutput("error", "system", f"æ‰§è¡Œå‡ºé”™: {e}"))
            finally:
                await self._stream_callback(StreamOutput("end", "system", "æ‰§è¡Œå®Œæˆ"))
                stream_callback.reset(token)

        # å¯åŠ¨å›¾æ‰§è¡Œä»»åŠ¡
        task = asyncio.create_task(graph_task())

        # ä»é˜Ÿåˆ—ä¸­å¼‚æ­¥åœ°yieldè¾“å‡º
        while True:
            output = await self.output_queue.get()
            yield output
            if output.get("type") == "end":
                break
        
        await task


def get_research_topic(messages):
    """ä»æ¶ˆæ¯ä¸­è·å–ç ”ç©¶ä¸»é¢˜"""
    if not messages:
        return ""

    if len(messages) == 1:
        return messages[-1].content

    for message in reversed(messages):
        if hasattr(message, 'type') and message.type == 'human':
            return message.content
        if hasattr(message, 'role') and message.role == 'user':
            return message.content

    return messages[-1].content


# æµ‹è¯•ä»£ç 
async def main():
    agent = StreamingGraph()
    queries = ["æœç´¢ä¸Šæµ·å¤©æ°”", "æœç´¢æ·±åœ³å¤©æ°”"]
    for i, query in enumerate(queries, 1):
        print(f"\n--- ç¬¬{i}æ¬¡æŸ¥è¯¢: {query} ---")
        user_msg = HumanMessage(content=query)
        
        async for output in agent.run_with_streaming([user_msg]):
            output_type = output.get('type', 'unknown')
            node = output.get('node', 'unknown')
            content = output.get('content', '')

            if output_type == "streaming":
                print(f"{content}", end='', flush=True)
            elif output_type in ["start", "complete", "error"]:
                emoji = {"start": "ğŸš€", "complete": "âœ…", "error": "âŒ"}[output_type]
                print(f"\n{emoji} [{node}] {content}")
            elif output_type == "final_result":
                print(f"\nğŸ¯ æŸ¥è¯¢{i}å®Œæˆ")
            
            if output_type == 'end':
                break

if __name__ == "__main__":
    asyncio.run(main())