import os
from typing import Dict, List

from agentchat.services.deepsearch.tools_and_schemas import SearchQueryList, Reflection
from dotenv import load_dotenv
from langchain_core.messages import AIMessage
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from tavily import TavilyClient
from loguru import logger
import json

from agentchat.services.deepsearch.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
)
from agentchat.services.deepsearch.configuration import Configuration
from agentchat.services.deepsearch.prompts import (
    get_current_date,
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
)
from agentchat.core.models.manager import ModelManager

load_dotenv()

# æ£€æŸ¥Tavily APIå¯†é’¥
if os.getenv("TAVILY_API_KEY") is None:
    raise ValueError("TAVILY_API_KEY is not set")

# åˆå§‹åŒ–Tavilyå®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))


# èŠ‚ç‚¹
def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """LangGraphèŠ‚ç‚¹ï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€‚

    ä½¿ç”¨æœ¬åœ°LLMæ¨¡å‹åŸºäºç”¨æˆ·é—®é¢˜åˆ›å»ºä¼˜åŒ–çš„ç½‘ç»œç ”ç©¶æœç´¢æŸ¥è¯¢ã€‚

    å‚æ•°:
        state: åŒ…å«ç”¨æˆ·é—®é¢˜çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œçš„é…ç½®ï¼ŒåŒ…æ‹¬LLMæä¾›è€…è®¾ç½®

    è¿”å›:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬åŒ…å«ç”ŸæˆæŸ¥è¯¢çš„search_queryé”®
    """
    configurable = Configuration.from_runnable_config(config)

    # æ£€æŸ¥è‡ªå®šä¹‰åˆå§‹æœç´¢æŸ¥è¯¢è®¡æ•°
    if state.get("initial_search_query_count") is None:
        state["initial_search_query_count"] = configurable.number_of_initial_queries

    # ä½¿ç”¨ModelManagerè·å–æ¨¡å‹
    llm = ModelManager.get_conversation_model()

    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    research_topic = get_research_topic(state["messages"])
    
    formatted_prompt = f"""
    {query_writer_instructions.format(
        current_date=current_date,
        research_topic=research_topic,
        number_queries=state["initial_search_query_count"],
    )}
    
    è¯·ç”¨JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¤ä¸ªé”®:
    {{
        "rationale": "ç®€è¦è§£é‡Šè¿™äº›æŸ¥è¯¢ä¸ç ”ç©¶ä¸»é¢˜çš„ç›¸å…³æ€§",
        "query": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", ...]
    }}
    """
    
    # ç”Ÿæˆæœç´¢æŸ¥è¯¢
    response = llm.invoke(formatted_prompt)
    content = response.content
    
    # è§£æJSONå“åº”
    try:
        result = json.loads(content)
        queries = result.get("query", [])
        if not queries:
            # å¦‚æœæ²¡æœ‰æŸ¥è¯¢ï¼Œä½¿ç”¨åŸå§‹ç ”ç©¶ä¸»é¢˜ä½œä¸ºæŸ¥è¯¢
            queries = [research_topic]
        return {"search_query": queries}
    except:
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç ”ç©¶ä¸»é¢˜ä½œä¸ºæŸ¥è¯¢
        logger.error("è§£ææŸ¥è¯¢ç”Ÿæˆç»“æœå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜ä½œä¸ºæŸ¥è¯¢")
        return {"search_query": [research_topic]}


def continue_to_web_research(state: QueryGenerationState):
    """LangGraphèŠ‚ç‚¹ï¼Œå°†æœç´¢æŸ¥è¯¢å‘é€åˆ°ç½‘ç»œç ”ç©¶èŠ‚ç‚¹ã€‚

    ç”¨äºä¸ºæ¯ä¸ªæœç´¢æŸ¥è¯¢ç”Ÿæˆnä¸ªç½‘ç»œç ”ç©¶èŠ‚ç‚¹ã€‚
    """
    return [
        Send("web_research", {"search_query": search_query, "id": int(idx)})
        for idx, search_query in enumerate(state["search_query"])
    ]


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """LangGraphèŠ‚ç‚¹ï¼Œä½¿ç”¨Tavilyæœç´¢APIæ‰§è¡Œç½‘ç»œç ”ç©¶ã€‚

    æ‰§è¡Œç½‘ç»œæœç´¢å¹¶æ ¼å¼åŒ–ç»“æœã€‚

    å‚æ•°:
        state: åŒ…å«æœç´¢æŸ¥è¯¢å’ŒIDçš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œçš„é…ç½®

    è¿”å›:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬sources_gatheredå’Œweb_research_result
    """
    search_query = state["search_query"]
    query_id = state["id"]
    
    logger.info(f"ğŸ” æ‰§è¡Œæœç´¢: {search_query}")
    
    try:
        # ä½¿ç”¨Tavilyæ‰§è¡Œæœç´¢
        response = tavily_client.search(
            query=search_query,
            max_results=10,
            time_range="month",  # æ—¶é—´è·¨åº¦ä¸ºè¿‘ä¸€æœˆå†…çš„äº‹æƒ…
            include_raw_content="markdown",
            country="china"
        )
        
        # æ ¼å¼åŒ–æœç´¢ç»“æœ
        formatted_results = format_tavily_results(response)
        
        # åˆ›å»ºç®€å•çš„å¼•ç”¨æ ‡è®°
        sources = []
        for idx, result in enumerate(response.get("results", [])):
            source_id = f"{query_id}-{idx}"
            source_url = result.get("url", "")
            source_title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
            sources.append({
                "short_url": f"https://search.result/{source_id}",
                "value": source_url,
                "label": source_title
            })
        
        logger.info(f"âœ… æ‰¾åˆ° {len(response.get('results', []))} ä¸ªç»“æœ")
        
        return {
            "sources_gathered": sources,
            "search_query": [search_query],
            "web_research_result": [formatted_results],
        }
    
    except Exception as e:
        logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
        return {
            "sources_gathered": [],
            "search_query": [search_query],
            "web_research_result": [f"æœç´¢å¤±è´¥: {str(e)}"],
        }


def format_tavily_results(response: Dict) -> str:
    """æ ¼å¼åŒ–Tavilyæœç´¢ç»“æœ"""
    if not response.get("results"):
        return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"

    formatted = []
    for idx, result in enumerate(response["results"]):
        url = result.get("url", "")
        title = result.get("title", "")
        content = result.get("content", "")
        formatted.append(f"[{title}]({url})\nå†…å®¹: {content}")

    return "\n\n".join(formatted)


def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    """LangGraphèŠ‚ç‚¹ï¼Œè¯†åˆ«çŸ¥è¯†ç¼ºå£å¹¶ç”Ÿæˆæ½œåœ¨çš„åç»­æŸ¥è¯¢ã€‚

    åˆ†æå½“å‰æ‘˜è¦ä»¥è¯†åˆ«è¿›ä¸€æ­¥ç ”ç©¶çš„é¢†åŸŸï¼Œå¹¶ç”Ÿæˆæ½œåœ¨çš„åç»­æŸ¥è¯¢ã€‚
    ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºä»¥JSONæ ¼å¼æå–åç»­æŸ¥è¯¢ã€‚

    å‚æ•°:
        state: åŒ…å«è¿è¡Œæ‘˜è¦å’Œç ”ç©¶ä¸»é¢˜çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œçš„é…ç½®ï¼ŒåŒ…æ‹¬LLMæä¾›è€…è®¾ç½®

    è¿”å›:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬åŒ…å«ç”Ÿæˆçš„åç»­æŸ¥è¯¢çš„search_queryé”®
    """
    configurable = Configuration.from_runnable_config(config)
    # å¢åŠ ç ”ç©¶å¾ªç¯è®¡æ•°
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    
    # ä½¿ç”¨ModelManagerè·å–æ¨¡å‹
    llm = ModelManager.get_conversation_model()

    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    research_topic = get_research_topic(state["messages"])
    summaries = "\n\n---\n\n".join(state["web_research_result"])
    
    formatted_prompt = f"""
    {reflection_instructions.format(
        current_date=current_date,
        research_topic=research_topic,
        summaries=summaries,
    )}
    """
    
    # ç”Ÿæˆåæ€ç»“æœ
    response = llm.invoke(formatted_prompt)
    content = response.content
    
    # è§£æJSONå“åº”
    try:
        result = json.loads(content)
        is_sufficient = result.get("is_sufficient", True)
        knowledge_gap = result.get("knowledge_gap", "")
        follow_up_queries = result.get("follow_up_queries", [])
        
        logger.info(f"ğŸ“Š åæ€ç»“æœ: {'è¶³å¤Ÿ' if is_sufficient else 'ä¸è¶³å¤Ÿ'}")
        if not is_sufficient:
            logger.info(f"ğŸ’­ çŸ¥è¯†ç¼ºå£: {knowledge_gap}")
            logger.info(f"ğŸ”„ åç»­æŸ¥è¯¢: {follow_up_queries}")
        
        return {
            "is_sufficient": is_sufficient,
            "knowledge_gap": knowledge_gap,
            "follow_up_queries": follow_up_queries,
            "research_loop_count": state["research_loop_count"],
            "number_of_ran_queries": len(state["search_query"]),
        }
    except:
        logger.error("è§£æåæ€ç»“æœå¤±è´¥ï¼Œé»˜è®¤ä¸ºè¶³å¤Ÿ")
        return {
            "is_sufficient": True,
            "knowledge_gap": "",
            "follow_up_queries": [],
            "research_loop_count": state["research_loop_count"],
            "number_of_ran_queries": len(state["search_query"]),
        }


def evaluate_research(
    state: ReflectionState,
    config: RunnableConfig,
) -> OverallState:
    """LangGraphè·¯ç”±å‡½æ•°ï¼Œç¡®å®šç ”ç©¶æµç¨‹ä¸­çš„ä¸‹ä¸€æ­¥ã€‚

    é€šè¿‡å†³å®šæ˜¯ç»§ç»­æ”¶é›†ä¿¡æ¯è¿˜æ˜¯åŸºäºé…ç½®çš„æœ€å¤§ç ”ç©¶å¾ªç¯æ•°æ¥å®Œæˆæ‘˜è¦ï¼Œæ§åˆ¶ç ”ç©¶å¾ªç¯ã€‚

    å‚æ•°:
        state: åŒ…å«ç ”ç©¶å¾ªç¯è®¡æ•°çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œçš„é…ç½®ï¼ŒåŒ…æ‹¬max_research_loopsè®¾ç½®

    è¿”å›:
        æŒ‡ç¤ºè¦è®¿é—®çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„å­—ç¬¦ä¸²å­—é¢é‡æˆ–å‘é€åˆ°web_researchçš„æŸ¥è¯¢åˆ—è¡¨
    """
    configurable = Configuration.from_runnable_config(config)
    max_research_loops = (
        state.get("max_research_loops")
        if state.get("max_research_loops") is not None
        else configurable.max_research_loops
    )
    if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
        logger.info("âœ… ç ”ç©¶å®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        return "finalize_answer"
    else:
        logger.info("ğŸ”„ ç»§ç»­ç ”ç©¶ï¼Œæ‰§è¡Œåç»­æŸ¥è¯¢")
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state["number_of_ran_queries"] + int(idx),
                },
            )
            for idx, follow_up_query in enumerate(state["follow_up_queries"])
        ]


def finalize_answer(state: OverallState, config: RunnableConfig):
    """LangGraphèŠ‚ç‚¹ï¼Œå®Œæˆç ”ç©¶æ‘˜è¦ã€‚

    é€šè¿‡å»é‡å’Œæ ¼å¼åŒ–æºï¼Œç„¶åå°†å®ƒä»¬ä¸è¿è¡Œæ‘˜è¦ç»“åˆï¼Œåˆ›å»ºä¸€ä¸ªç»“æ„è‰¯å¥½çš„
    å¸¦æœ‰é€‚å½“å¼•ç”¨çš„ç ”ç©¶æŠ¥å‘Šï¼Œå‡†å¤‡æœ€ç»ˆè¾“å‡ºã€‚

    å‚æ•°:
        state: åŒ…å«è¿è¡Œæ‘˜è¦å’Œæ”¶é›†çš„æºçš„å½“å‰å›¾çŠ¶æ€

    è¿”å›:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬åŒ…å«æ ¼å¼åŒ–æœ€ç»ˆæ‘˜è¦å’Œæºçš„running_summaryé”®
    """
    # ä½¿ç”¨ModelManagerè·å–æ¨¡å‹
    llm = ModelManager.get_conversation_model()

    # æ ¼å¼åŒ–æç¤ºè¯
    current_date = get_current_date()
    research_topic = get_research_topic(state["messages"])
    summaries = "\n---\n\n".join(state["web_research_result"])
    
    formatted_prompt = f"""
    {answer_instructions.format(
        current_date=current_date,
        research_topic=research_topic,
        summaries=summaries,
    )}
    """
    
    # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    response = llm.invoke(formatted_prompt)
    content = response.content
    
    logger.info("ğŸ¯ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå®Œæˆ")
    
    # å°†çŸ­URLæ›¿æ¢ä¸ºåŸå§‹URL
    unique_sources = []
    for source in state["sources_gathered"]:
        if source["short_url"] in content:
            content = content.replace(source["short_url"], source["value"])
            unique_sources.append(source)

    return {
        "messages": [AIMessage(content=content)],
        "sources_gathered": unique_sources,
    }


def get_research_topic(messages):
    """ä»æ¶ˆæ¯ä¸­è·å–ç ”ç©¶ä¸»é¢˜"""
    if not messages:
        return ""
    
    # å¦‚æœåªæœ‰ä¸€æ¡æ¶ˆæ¯ï¼Œç›´æ¥è¿”å›å†…å®¹
    if len(messages) == 1:
        return messages[-1].content
    
    # å¦åˆ™ï¼Œç»„åˆæœ€è¿‘çš„ç”¨æˆ·æ¶ˆæ¯
    for message in reversed(messages):
        if hasattr(message, 'type') and message.type == 'human':
            return message.content
        if hasattr(message, 'role') and message.role == 'user':
            return message.content
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯ï¼Œè¿”å›æœ€åä¸€æ¡æ¶ˆæ¯
    return messages[-1].content


# åˆ›å»ºæˆ‘ä»¬çš„æ™ºèƒ½ä½“å›¾
builder = StateGraph(OverallState, config_schema=Configuration)

# å®šä¹‰æˆ‘ä»¬å°†å¾ªç¯çš„èŠ‚ç‚¹
builder.add_node("generate_query", generate_query)
builder.add_node("web_research", web_research)
builder.add_node("reflection", reflection)
builder.add_node("finalize_answer", finalize_answer)

# å°†å…¥å£ç‚¹è®¾ç½®ä¸º`generate_query`
# è¿™æ„å‘³ç€è¿™ä¸ªèŠ‚ç‚¹æ˜¯ç¬¬ä¸€ä¸ªè¢«è°ƒç”¨çš„
builder.add_edge(START, "generate_query")
# æ·»åŠ æ¡ä»¶è¾¹ä»¥åœ¨å¹¶è¡Œåˆ†æ”¯ä¸­ç»§ç»­æœç´¢æŸ¥è¯¢
builder.add_conditional_edges(
    "generate_query", continue_to_web_research, ["web_research"]
)
# å¯¹ç½‘ç»œç ”ç©¶è¿›è¡Œåæ€
builder.add_edge("web_research", "reflection")
# è¯„ä¼°ç ”ç©¶
builder.add_conditional_edges(
    "reflection", evaluate_research, ["web_research", "finalize_answer"]
)
# å®Œæˆç­”æ¡ˆ
builder.add_edge("finalize_answer", END)

graph = builder.compile(name="pro-search-agent")